{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision.transforms as T \n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 10,\n",
       "  'id': 10,\n",
       "  'category_id': 1,\n",
       "  'bbox': [47, 161, 616, 468],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Pain', 'Sadness', 'Sensitivity', 'Suffering'],\n",
       "  'annotations_continuous': {'valence': 2, 'arousal': 7, 'dominance': 4},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Kid'},\n",
       " {'image_id': 10,\n",
       "  'id': 11,\n",
       "  'category_id': 1,\n",
       "  'bbox': [177, 11, 570, 471],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Sadness', 'Suffering', 'Sympathy'],\n",
       "  'annotations_continuous': {'valence': 2, 'arousal': 3, 'dominance': 4},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 10,\n",
       "  'id': 12,\n",
       "  'category_id': 1,\n",
       "  'bbox': [200, 182, 344, 471],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Fear', 'Sadness', 'Suffering'],\n",
       "  'annotations_continuous': {'valence': 2, 'arousal': 4, 'dominance': 2},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Kid'},\n",
       " {'image_id': 11,\n",
       "  'id': 13,\n",
       "  'category_id': 1,\n",
       "  'bbox': [157, 43, 470, 420],\n",
       "  'coco_ids': {'image_id': 547457, 'annotations_id': 424485},\n",
       "  'annotations_categories': ['Anticipation',\n",
       "   'Engagement',\n",
       "   'Excitement',\n",
       "   'Happiness'],\n",
       "  'annotations_continuous': {'valence': 8, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 12,\n",
       "  'id': 14,\n",
       "  'category_id': 1,\n",
       "  'bbox': [246, 63, 458, 380],\n",
       "  'coco_ids': {'image_id': 122997, 'annotations_id': 558850},\n",
       "  'annotations_categories': ['Engagement'],\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 3, 'dominance': 8},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 12,\n",
       "  'id': 15,\n",
       "  'category_id': 1,\n",
       "  'bbox': [141, 47, 348, 343],\n",
       "  'coco_ids': {'image_id': 122997, 'annotations_id': 558850},\n",
       "  'annotations_categories': ['Disconnection', 'Engagement', 'Peace'],\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 13,\n",
       "  'id': 16,\n",
       "  'category_id': 1,\n",
       "  'bbox': [34, 26, 499, 304],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Engagement'],\n",
       "  'annotations_continuous': {'valence': 7, 'arousal': 4, 'dominance': 2},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 14,\n",
       "  'id': 17,\n",
       "  'category_id': 1,\n",
       "  'bbox': [148, 8, 471, 616],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Happiness'],\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 4, 'dominance': 9},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 15,\n",
       "  'id': 18,\n",
       "  'category_id': 1,\n",
       "  'bbox': [28, 240, 201, 472],\n",
       "  'coco_ids': {'image_id': 179164, 'annotations_id': 201513},\n",
       "  'annotations_categories': ['Yearning'],\n",
       "  'annotations_continuous': {'valence': 2, 'arousal': 3, 'dominance': 3},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 16,\n",
       "  'id': 19,\n",
       "  'category_id': 1,\n",
       "  'bbox': [182, 83, 386, 445],\n",
       "  'coco_ids': {'image_id': 121006, 'annotations_id': 475911},\n",
       "  'annotations_categories': ['Confidence', 'Happiness'],\n",
       "  'annotations_continuous': {'valence': 8, 'arousal': 6, 'dominance': 6},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Teenager'},\n",
       " {'image_id': 17,\n",
       "  'id': 20,\n",
       "  'category_id': 1,\n",
       "  'bbox': [497, 89, 625, 436],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Engagement'],\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 7, 'dominance': 6},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 18,\n",
       "  'id': 21,\n",
       "  'category_id': 1,\n",
       "  'bbox': [158, 16, 378, 304],\n",
       "  'coco_ids': {'image_id': 140322, 'annotations_id': 460502},\n",
       "  'annotations_categories': ['Happiness'],\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 7, 'dominance': 3},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Kid'},\n",
       " {'image_id': 18,\n",
       "  'id': 22,\n",
       "  'category_id': 1,\n",
       "  'bbox': [83, 146, 608, 511],\n",
       "  'coco_ids': {'image_id': 140322, 'annotations_id': 460502},\n",
       "  'annotations_categories': ['Anticipation', 'Engagement', 'Happiness'],\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 3},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Kid'},\n",
       " {'image_id': 19,\n",
       "  'id': 23,\n",
       "  'category_id': 1,\n",
       "  'bbox': [6, 71, 350, 488],\n",
       "  'coco_ids': {'image_id': 497759, 'annotations_id': 486921},\n",
       "  'annotations_categories': ['Disquietment'],\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 7, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 20,\n",
       "  'id': 24,\n",
       "  'category_id': 1,\n",
       "  'bbox': [126, 70, 344, 439],\n",
       "  'coco_ids': {'image_id': 1966, 'annotations_id': 484353},\n",
       "  'annotations_categories': ['Anticipation', 'Confidence', 'Engagement'],\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 9, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 21,\n",
       "  'id': 25,\n",
       "  'category_id': 1,\n",
       "  'bbox': [381, 92, 493, 388],\n",
       "  'coco_ids': {'image_id': 326320, 'annotations_id': 519731},\n",
       "  'annotations_categories': ['Yearning'],\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 22,\n",
       "  'id': 26,\n",
       "  'category_id': 1,\n",
       "  'bbox': [209, 117, 318, 335],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Anticipation', 'Disquietment', 'Engagement'],\n",
       "  'annotations_continuous': {'valence': 2, 'arousal': 7, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Kid'},\n",
       " {'image_id': 23,\n",
       "  'id': 27,\n",
       "  'category_id': 1,\n",
       "  'bbox': [2035, 654, 2176, 921],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Engagement'],\n",
       "  'annotations_continuous': {'valence': 7, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 23,\n",
       "  'id': 28,\n",
       "  'category_id': 1,\n",
       "  'bbox': [1757, 625, 1900, 910],\n",
       "  'coco_ids': [],\n",
       "  'annotations_categories': ['Disconnection'],\n",
       "  'annotations_continuous': {'valence': 'None',\n",
       "   'arousal': 'None',\n",
       "   'dominance': 'None'},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 24,\n",
       "  'id': 29,\n",
       "  'category_id': 1,\n",
       "  'bbox': [73, 276, 276, 493],\n",
       "  'coco_ids': {'image_id': 253959, 'annotations_id': 197346},\n",
       "  'annotations_categories': ['Happiness', 'Peace'],\n",
       "  'annotations_continuous': {'valence': 7, 'arousal': 8, 'dominance': 2},\n",
       "  'gender': 'Female',\n",
       "  'age': 'Teenager'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './new_annotations/EMOTIC_train_x1y1x2y2.json'\n",
    "train = json.load(open(path))\n",
    "train_anno = train['annotations'] # dictionnary of annotations\n",
    "train_img = train['images'] # dictionnary of images\n",
    "train_anno[10:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images dictionnary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\JALAL/.cache\\torch\\hub\\facebookresearch_detr_main\n",
      "c:\\Users\\JALAL\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\JALAL\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DETR(\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (class_embed): Linear(in_features=256, out_features=92, bias=True)\n",
       "  (bbox_embed): MLP(\n",
       "    (layers): ModuleList(\n",
       "      (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "      (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (query_embed): Embedding(100, 256)\n",
       "  (input_proj): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (backbone): Joiner(\n",
       "    (0): Backbone(\n",
       "      (body): IntermediateLayerGetter(\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "        (bn1): FrozenBatchNorm2d()\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        (layer1): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (3): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (4): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (5): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): Bottleneck(\n",
       "            (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): FrozenBatchNorm2d()\n",
       "            )\n",
       "          )\n",
       "          (1): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): Bottleneck(\n",
       "            (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn1): FrozenBatchNorm2d()\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): FrozenBatchNorm2d()\n",
       "            (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (bn3): FrozenBatchNorm2d()\n",
       "            (relu): ReLU(inplace=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): PositionEmbeddingSine()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing model \n",
    "model = torch.hub.load('facebookresearch/detr:main', 'detr_resnet50', pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_results(img):\n",
    "    img = transform(img).unsqueeze(0)\n",
    "    outputs = model(img)\n",
    "    # keep only predictions with 0.9+ confidence and labeled as \"person\"\n",
    "    probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "    keep = (probas.max(-1).values > 0.9) & (probas.argmax(-1) == 1)  # Filter for \"person\" class\n",
    "    # convert boxes from [0; 1] to image scales\n",
    "    # Correcting the line causing TypeError\n",
    "    bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], img.size()[2:])\n",
    "    bboxes_scaled = bboxes_scaled.tolist()\n",
    "    return bboxes_scaled , probas[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': 0, 'bboxes': [[104.82051086425781, 79.68885803222656, 705.2249145507812, 796.5303344726562]]}, {'id': 1, 'bboxes': [[558.7202758789062, 853.2990112304688, 680.6359252929688, 1065.699462890625], [756.3778686523438, 333.6680603027344, 793.0950317382812, 502.22509765625], [743.842041015625, 297.0343322753906, 800.0303344726562, 839.23779296875], [188.13223266601562, 364.7883605957031, 262.90679931640625, 667.5501098632812], [200.64874267578125, 211.2132568359375, 420.8241882324219, 1059.3511962890625], [757.4473266601562, 337.278564453125, 783.6183471679688, 415.2267761230469], [338.9683837890625, 161.04525756835938, 541.1212158203125, 1009.454833984375], [501.7662658691406, 223.66307067871094, 574.97802734375, 698.0384521484375], [604.4768676757812, 332.0246276855469, 760.1392211914062, 1048.1500244140625]]}, {'id': 2, 'bboxes': [[619.4775390625, 829.3400268554688, 800.140869140625, 1060.1058349609375], [12.161705017089844, 704.91845703125, 23.83639144897461, 745.393798828125], [386.17138671875, 207.0525665283203, 576.8663940429688, 1018.915283203125]]}, {'id': 3, 'bboxes': [[476.1585693359375, 64.81621551513672, 1005.7522583007812, 453.3067626953125]]}, {'id': 4, 'bboxes': [[691.8379516601562, 1.371482491493225, 791.7774658203125, 505.7472229003906], [644.2593383789062, 7.630322456359863, 697.4579467773438, 153.39495849609375], [402.35552978515625, 32.048667907714844, 491.479736328125, 348.37548828125], [0.021649152040481567, 98.5657730102539, 22.034746170043945, 265.7537536621094], [517.6057739257812, -0.4477294981479645, 577.5111083984375, 375.3480529785156], [251.62820434570312, 11.614945411682129, 326.086181640625, 317.2427062988281], [336.075439453125, 26.50761604309082, 412.7596435546875, 324.82196044921875], [69.50244903564453, 521.7860717773438, 241.89016723632812, 1041.5384521484375], [465.196044921875, 63.31766128540039, 524.8037109375, 364.6675720214844], [311.77923583984375, 745.2225341796875, 471.7294616699219, 1060.1019287109375], [440.35211181640625, 653.9740600585938, 636.6839599609375, 1030.71826171875]]}, {'id': 5, 'bboxes': [[54.67333984375, 54.53909683227539, 517.1353149414062, 1064.7894287109375]]}, {'id': 6, 'bboxes': [[546.3583984375, 0.19178317487239838, 652.2625732421875, 230.40771484375], [-0.12309551239013672, 100.15702819824219, 538.2885131835938, 1062.7567138671875], [419.924072265625, 113.49551391601562, 564.8094482421875, 490.7041320800781]]}, {'id': 7, 'bboxes': [[326.8382873535156, 402.4642028808594, 447.1345520019531, 1014.336181640625], [420.1592102050781, 231.62216186523438, 622.5027465820312, 947.2341918945312]]}, {'id': 8, 'bboxes': [[235.3423614501953, 233.73129272460938, 474.654541015625, 862.6996459960938]]}, {'id': 9, 'bboxes': [[652.3648681640625, 44.493080139160156, 982.4400634765625, 378.0718688964844]]}, {'id': 10, 'bboxes': [[64.54601287841797, 355.9117736816406, 789.242431640625, 1051.5401611328125], [253.12771606445312, 14.682027816772461, 745.9654541015625, 1050.2515869140625]]}, {'id': 11, 'bboxes': [[203.6549530029297, 124.31192016601562, 599.536865234375, 1190.8260498046875]]}]\n"
     ]
    }
   ],
   "source": [
    "original_path = \"EMOTIC (1)/EMOTIC/PAMI/emotic\"\n",
    "# liste d'appairement des images et des annotations en dictionnaires\n",
    "list_appair = []\n",
    "i = 0\n",
    "for image in train_img:\n",
    "    # image est le dictionnaire d'information d'une image\n",
    "    # train image est le dictionnaire d'information de toutes les images\n",
    "    if i<12:\n",
    "        file_name = image['file_name']\n",
    "        folder = image['folder']\n",
    "        img_path = original_path + '/' + folder + '/' + file_name\n",
    "        img = Image.open(img_path)\n",
    "        bboxes , probas = model_results(img)\n",
    "        list_appair.append({'id': image['id'], 'bboxes': bboxes})\n",
    "    i+=1\n",
    "print(list_appair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotations dictionnary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(box1, box2):\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1g, y1g, x2g, y2g = box2\n",
    "    # determine the coordinates of the intersection rectangle\n",
    "    xA = max(x1, x1g)\n",
    "    yA = max(y1, y1g)\n",
    "    xB = min(x2, x2g)\n",
    "    yB = min(y2, y2g)\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "    # compute the area of both the prediction and ground-truth\n",
    "    # rectangles\n",
    "    boxAArea = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    boxBArea = (x2g - x1g + 1) * (y2g - y1g + 1)\n",
    "    # compute the intersection over union by taking the intersection\n",
    "    # area and dividing it by the sum of prediction + ground-truth\n",
    "    # areas - the interesection area\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    # return the intersection over union value\n",
    "    return iou\n",
    "\n",
    "def get_iou(bbox1, bbox2, thresh, new_annots = []):\n",
    "    iou_score = iou(bbox1, bbox2)\n",
    "    if iou_score < thresh:\n",
    "        new_annots.append(bbox1)\n",
    "        new_annots.append(bbox2)\n",
    "    else:\n",
    "        pass\n",
    "    return new_annots\n",
    "\n",
    "def remove_duplicates(lst):\n",
    "    return [list(t) for t in {tuple(item) for item in lst}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86, 58, 564, 628]\n"
     ]
    }
   ],
   "source": [
    "print(train_anno[0]['bbox'])\n",
    "train_new_annots = train_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image_id': 0,\n",
       "  'id': 0,\n",
       "  'category_id': 1,\n",
       "  'bbox': [104.82051086425781,\n",
       "   79.68885803222656,\n",
       "   705.2249145507812,\n",
       "   796.5303344726562],\n",
       "  'coco_ids': {'image_id': 562243, 'annotations_id': 448867},\n",
       "  'annotations_categories': 'Disconnection',\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 3, 'dominance': 9},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 0,\n",
       "  'id': 1,\n",
       "  'category_id': 1,\n",
       "  'bbox': [86, 58, 564, 628],\n",
       "  'coco_ids': {'image_id': 562243, 'annotations_id': 448867},\n",
       "  'annotations_categories': 'Doubt/Confusion',\n",
       "  'annotations_continuous': {'valence': 5, 'arousal': 3, 'dominance': 9},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 2,\n",
       "  'category_id': 1,\n",
       "  'bbox': [338.9683837890625,\n",
       "   161.04525756835938,\n",
       "   541.1212158203125,\n",
       "   1009.454833984375],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': 'Anticipation',\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 3,\n",
       "  'category_id': 1,\n",
       "  'bbox': [200.64874267578125,\n",
       "   211.2132568359375,\n",
       "   420.8241882324219,\n",
       "   1059.3511962890625],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 4,\n",
       "  'category_id': 1,\n",
       "  'bbox': [485, 149, 605, 473],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 5,\n",
       "  'category_id': 1,\n",
       "  'bbox': [604.4768676757812,\n",
       "   332.0246276855469,\n",
       "   760.1392211914062,\n",
       "   1048.1500244140625],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 6,\n",
       "  'category_id': 1,\n",
       "  'bbox': [757.4473266601562,\n",
       "   337.278564453125,\n",
       "   783.6183471679688,\n",
       "   415.2267761230469],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 7,\n",
       "  'category_id': 1,\n",
       "  'bbox': [558.7202758789062,\n",
       "   853.2990112304688,\n",
       "   680.6359252929688,\n",
       "   1065.699462890625],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 8,\n",
       "  'category_id': 1,\n",
       "  'bbox': [501.7662658691406,\n",
       "   223.66307067871094,\n",
       "   574.97802734375,\n",
       "   698.0384521484375],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'},\n",
       " {'image_id': 1,\n",
       "  'id': 9,\n",
       "  'category_id': 1,\n",
       "  'bbox': [756.3778686523438,\n",
       "   333.6680603027344,\n",
       "   793.0950317382812,\n",
       "   502.22509765625],\n",
       "  'coco_ids': {'image_id': 288841, 'annotations_id': 1750456},\n",
       "  'annotations_categories': None,\n",
       "  'annotations_continuous': {'valence': 6, 'arousal': 4, 'dominance': 7},\n",
       "  'gender': 'Male',\n",
       "  'age': 'Adult'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "new_annotations = []\n",
    "new_id = 0\n",
    "\n",
    "for i, anno in enumerate(train_new_annots):\n",
    "    img_id = anno['image_id']\n",
    "    category_id = anno['category_id']\n",
    "    bbox = anno['bbox']\n",
    "    if not isinstance(bbox[0], list):\n",
    "        bbox = [bbox]  # Assurez-vous que bbox est une liste de listes\n",
    "    \n",
    "    # Trouver l'appariement pour cet image_id\n",
    "    for appair in list_appair:\n",
    "        if i<12:\n",
    "            if appair['id'] == img_id:\n",
    "                new_annots = []\n",
    "                # Comparer chaque bbox à ceux dans appair et ajuster selon get_iou\n",
    "                for single_bbox in bbox:\n",
    "                    for bbox2 in appair['bboxes']:\n",
    "                        new_annots = get_iou(single_bbox, bbox2, 0.99, new_annots)\n",
    "                new_bbox = remove_duplicates(new_annots)\n",
    "    \n",
    "    # S'assurer que annotations_categories est ajusté si nécessaire\n",
    "    extended_categories = anno['annotations_categories'][:]\n",
    "    if len(new_bbox) > len(extended_categories):\n",
    "        extended_categories.extend([None] * (len(new_bbox) - len(extended_categories)))\n",
    "    \n",
    "    # Créer une nouvelle annotation pour chaque bbox ajusté\n",
    "    for j, single_bbox in enumerate(new_bbox):\n",
    "        new_anno = {\n",
    "            'image_id': img_id,\n",
    "            'id': new_id,\n",
    "            'category_id': category_id,\n",
    "            'bbox': single_bbox,\n",
    "            'coco_ids': anno['coco_ids'],\n",
    "            'annotations_categories': extended_categories[j] if j < len(extended_categories) else None,\n",
    "            'annotations_continuous': anno['annotations_continuous'],\n",
    "            'gender': anno['gender'],\n",
    "            'age': anno['age'],\n",
    "        }\n",
    "        new_annotations.append(new_anno)\n",
    "        new_id += 1\n",
    "\n",
    "new_annotations[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor i, anno in enumerate(train_new_annots):\\n    if i < 5:\\n        img_id = anno['image_id']\\n        anno_id = anno['id']\\n        bbox = anno['bbox']\\n        \\n        # Check if bbox is not already a list of lists\\n        if not isinstance(bbox[0], list):\\n            bbox = [bbox]\\n        \\n        print(img_id, anno_id, bbox)\\n        \\n        for appair in list_appair:\\n            if appair['id'] == img_id:\\n                # Initialize new annotations list for managing bboxes for each image\\n                new_annots = []\\n                \\n                for single_bbox in bbox:  # Use single_bbox to avoid confusion with the outer bbox\\n                    for bbox2 in appair['bboxes']:\\n                        new_annots = get_iou(single_bbox, bbox2, 0.99, new_annots)\\n                \\n                anno['bbox'] = remove_duplicates(new_annots)\\n        # Extend the annotations categories list with None for the remaining bboxes\\n        if len(train_anno[i]['bbox']) > len(train_new_annots[i]['annotations_categories']):\\n            train_new_annots[i]['annotations_categories'].extend([None] * (len(train_anno[i]['bbox']) - len(train_new_annots[i]['annotations_categories'])))\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i, anno in enumerate(train_new_annots):\n",
    "    if i < 5:\n",
    "        img_id = anno['image_id']\n",
    "        anno_id = anno['id']\n",
    "        bbox = anno['bbox']\n",
    "        \n",
    "        # Check if bbox is not already a list of lists\n",
    "        if not isinstance(bbox[0], list):\n",
    "            bbox = [bbox]\n",
    "        \n",
    "        print(img_id, anno_id, bbox)\n",
    "        \n",
    "        for appair in list_appair:\n",
    "            if appair['id'] == img_id:\n",
    "                # Initialize new annotations list for managing bboxes for each image\n",
    "                new_annots = []\n",
    "                \n",
    "                for single_bbox in bbox:  # Use single_bbox to avoid confusion with the outer bbox\n",
    "                    for bbox2 in appair['bboxes']:\n",
    "                        new_annots = get_iou(single_bbox, bbox2, 0.99, new_annots)\n",
    "                \n",
    "                anno['bbox'] = remove_duplicates(new_annots)\n",
    "        # Extend the annotations categories list with None for the remaining bboxes\n",
    "        if len(train_anno[i]['bbox']) > len(train_new_annots[i]['annotations_categories']):\n",
    "            train_new_annots[i]['annotations_categories'].extend([None] * (len(train_anno[i]['bbox']) - len(train_new_annots[i]['annotations_categories'])))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './new_annotations/EMOTIC_train_x1y1x2y2.json'\n",
    "filename = './newest' + os.path.basename(path)\n",
    "\n",
    "# Create a dictionary with the images and annotations\n",
    "mixed_data = {'images': train_img, 'annotations': new_annotations}\n",
    "\n",
    "# Save the mixed data as a JSON file\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(mixed_data, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['images', 'annotations'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newest = json.load(open(filename))\n",
    "newest.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
